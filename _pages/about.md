---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Welcome to my personal website!  I am Yaojie Zhang , a senior majoring in Software Engineering at University of Electronic Science and Technology of China.

# ğŸ”¥ News
- *2025.10*: &nbsp;ğŸ‰ We released the paper "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM Inference".
- *2025.07*: &nbsp;ğŸ‰ We released the paper "Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles".
- *2025.05*: &nbsp;ğŸ‰ We released the paper "dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching".
- *2025.03*: &nbsp;ğŸ¤— Started Research Intern at EPIC Lab, Shanghai Jiao Tong University, focusing on Efficient Inference Methods.

# ğŸ“ Publications (* denotes the equal contribution.)

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/maskKV.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM Inference](https://arxiv.org/abs/2510.09309)

Jianuo Huang *, **Yaojie Zhang** *, Yicun Yang, Benhao Huang, Biqing Qi, Dongrui Liu, Linfeng Zhang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/slowfast-sampling.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles](https://arxiv.org/abs/2506.10848)

Qingyan Wei, **Yaojie Zhang**, Zhiyuan Liu, Dongrui Liu, Linfeng Zhang.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/dLLM-Cache_pipeline.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)

Zhiyuan Liu *, Yicun Yang *, **Yaojie Zhang**, Junjie Chen, Chang Zou, Qingyan Wei, Shaobo Wang, Linfeng Zhang.

</div>
</div>

# ğŸ– Honors and Awards
- *2025* National Scholarship (å›½å®¶å¥–å­¦é‡‘)
- *2024* National Scholarship (å›½å®¶å¥–å­¦é‡‘)
- *2024* Virtuos Corporate Scholarship (ç»´å¡”å£«ä¼ä¸šå¥–å­¦é‡‘)

# ğŸ“– Educations
- *2023.08 - 2027.06*,  University of Electronic Science and Technology of China, Bachelor of Software Engineering

# ğŸ’» Internships
- *2025.03 - Present*, Research Intern, EPIC Lab, Shanghai Jiao Tong University (Advised by Prof. [Linfeng Zhang](http://www.zhanglinfeng.tech/)).

